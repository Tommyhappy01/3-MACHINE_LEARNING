{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hi all.  üôã\n",
    "\n",
    "#### We continue our **Beginner-Intermediate Friendly Machine Learning series**, which would help anyone who wants to learn or refresh the basics of ML.\n",
    "\n",
    "#### What we have covered: \n",
    "\n",
    "#### [Beginner Friendly Detailed Explained EDAs ‚Äì For anyone at the beginnings of DS/ML journey](https://www.kaggle.com/general/253911#1393015) ‚úîÔ∏è\n",
    "\n",
    "#### [BIAS & VARIANCE TRADEOFF](https://www.kaggle.com/kaanboke/ml-basics-bias-variance-tradeoff) ‚úîÔ∏è\n",
    "\n",
    "#### [LINEAR ALGORITHMS](https://www.kaggle.com/kaanboke/ml-basics-linear-algorithms)  ‚úîÔ∏è\n",
    "\n",
    "#### [NONLINEAR ALGORITHMS](https://www.kaggle.com/kaanboke/nonlinear-algorithms)  ‚úîÔ∏è\n",
    "\n",
    "#### [The Most Used Methods to Deal with MISSING VALUES](https://www.kaggle.com/kaanboke/the-most-used-methods-to-deal-with-missing-values)  ‚úîÔ∏è\n",
    "\n",
    "#### [Beginner Friendly End to End ML Project- Classification with Imbalanced Data](https://www.kaggle.com/kaanboke/beginner-friendly-end-to-end-ml-project-enjoy)  ‚úîÔ∏è\n",
    "\n",
    "#### [How to Prevent the Data Leakage ?](https://www.kaggle.com/kaanboke/how-to-prevent-the-data-leakage) ‚úîÔ∏è\n",
    "\n",
    "#### [The Most Common EVALUATION METRICS- A Gentle Intro](https://www.kaggle.com/kaanboke/the-most-common-evaluation-metrics-a-gentle-intro) ‚úîÔ∏è\n",
    "\n",
    "\n",
    "#### In this notebook we will  cover one of the important concepts of the Data Science Journey : **Feature Selection**\n",
    "#### Enjoy ü§ò"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/1400/1*FUZS9K4JPqzfXDcC83BQTw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Credit: https://miro.medium.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc\"></a>\n",
    "\n",
    "<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Table of Contents</h3>\n",
    "    \n",
    "* [What is Feature Selection](#0)   \n",
    "* [Data](#1)\n",
    "* [Univariate Statistics-Filter Methods](#2)\n",
    "    * [Removing features with low variance - Variance Threshold](#3)\n",
    "    * [Select KBest for Classification Problems](#4)\n",
    "    * [Select KBest for Regression Problems](#5)\n",
    "    * [Information Gain - Classification Problems](#6)\n",
    "    * [Information Gain- Regression Problems](#7)\n",
    "    * [Select Percentile](#8)\n",
    "\n",
    "\n",
    "* [Model-Based Feature Selection - Embedded](#9)\n",
    "* [Iterative Feature Selection - Wrapper](#10)\n",
    "* [Conclusion](#11)\n",
    "* [References & Further Reading](#12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"0\"></a>\n",
    "<font color=\"lightseagreen\" size=+2.5><b>What is Feature Selection ?</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In machine learning and statistics, feature selection, also known as variable selection, attribute selection or variable subset selection, is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. Feature selection techniques are used for several reasons:\n",
    "\n",
    "> * simplification of models to make them easier to interpret by researchers/users,\n",
    "> * shorter training times,\n",
    "> * to avoid the curse of dimensionality,\n",
    "> * improve data's compatibility with a learning model class,\n",
    "> * encode inherent symmetries present in the input space.\n",
    "\n",
    "Reference: https://en.wikipedia.org/wiki/Feature_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://assets-global.website-files.com/5debb9b4f88fbc3f702d579e/60ecb081507f4559c84381f5_feature-selection-graphic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image credit: https://www.omnisci.com/technical-glossary/feature-selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though there are unsupervised feature selection techniques, in this study we will focus on the supervised feature selection techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the three  main approaches of feature selection in the supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.researchgate.net/profile/Enis-Karaarslan/publication/337591149/figure/fig2/AS:830089595990017@1574920190654/The-main-feature-selection-methods-for-machine-learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Credit: https://www.researchgate.net/profile/Enis-Karaarslan/publication/337591149"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T15:48:51.067498Z",
     "iopub.status.busy": "2021-09-23T15:48:51.066029Z",
     "iopub.status.idle": "2021-09-23T15:48:51.109314Z",
     "shell.execute_reply": "2021-09-23T15:48:51.107067Z",
     "shell.execute_reply.started": "2021-09-23T15:48:51.067428Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest,SelectPercentile,f_classif,f_regression,mutual_info_regression,mutual_info_classif,SelectFromModel,RFE\n",
    "\n",
    "pd.set_option('max_columns',100)\n",
    "pd.set_option('max_rows',900)\n",
    "\n",
    "pd.set_option('max_colwidth',200)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "<font color=\"lightseagreen\" size=+2.5><b>Data</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this study I'll use two dataset. One for classification problems and other one for the prediction problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heart Failure Clinical Records Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T15:48:51.113867Z",
     "iopub.status.busy": "2021-09-23T15:48:51.11248Z",
     "iopub.status.idle": "2021-09-23T15:48:51.167193Z",
     "shell.execute_reply": "2021-09-23T15:48:51.165896Z",
     "shell.execute_reply.started": "2021-09-23T15:48:51.113796Z"
    }
   },
   "outputs": [],
   "source": [
    "df_heart = pd.read_csv('../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv')\n",
    "\n",
    "df_heart.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Car Price Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T15:48:51.169188Z",
     "iopub.status.busy": "2021-09-23T15:48:51.168936Z",
     "iopub.status.idle": "2021-09-23T15:48:51.198189Z",
     "shell.execute_reply": "2021-09-23T15:48:51.197167Z",
     "shell.execute_reply.started": "2021-09-23T15:48:51.169161Z"
    }
   },
   "outputs": [],
   "source": [
    "df_car = pd.read_csv('../input/car-price-prediction/CarPrice_Assignment.csv', usecols = ['price', 'wheelbase', 'carlength', 'carwidth', 'carheight','curbweight', 'enginesize', 'boreratio', 'stroke', 'horsepower','peakrpm', 'citympg', 'highwaympg'])\n",
    "df_car.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "<font color=\"lightseagreen\" size=+2.5><b>Univariate Statistics - Filter Methods</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our main aim to find out statistically significant / meaningful relationship between features and target.\n",
    "- In this part, we will look at the different methods under the filter methods.\n",
    "    - Removing features with low variance - Variance Threshold\n",
    "    - KBest models for both classification and regression problems\n",
    "    - Information gain for both classification and regression problems\n",
    "    - Select percentile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "<font color=\"lightseagreen\" size=+1><b>Removing features with low variance - Variance Threshold</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance threshold allows us to set a minimum threshold for an accepted variance in each feature. As a default it removes all zero-variance features (same value in all samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T15:48:51.199971Z",
     "iopub.status.busy": "2021-09-23T15:48:51.199731Z",
     "iopub.status.idle": "2021-09-23T15:48:51.233515Z",
     "shell.execute_reply": "2021-09-23T15:48:51.230891Z",
     "shell.execute_reply.started": "2021-09-23T15:48:51.199943Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "X= df_heart.drop('DEATH_EVENT', axis=1)\n",
    "y= df_heart['DEATH_EVENT']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.30)\n",
    "\n",
    "\n",
    "vth = VarianceThreshold(threshold=0)  # as deafult threshold=0\n",
    "vth.fit(X_train)\n",
    "X_train_vth = X_train.iloc[:, vth.get_support()]\n",
    "\n",
    "pd.DataFrame( {'Feature': X_train.columns,'Variance': vth.variances_,}).sort_values('Variance', ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T15:48:51.237195Z",
     "iopub.status.busy": "2021-09-23T15:48:51.236344Z",
     "iopub.status.idle": "2021-09-23T15:48:51.268426Z",
     "shell.execute_reply": "2021-09-23T15:48:51.267282Z",
     "shell.execute_reply.started": "2021-09-23T15:48:51.237144Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "X= df_car.drop('price', axis=1)\n",
    "y= df_car['price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.30)\n",
    "\n",
    "\n",
    "vth = VarianceThreshold(threshold=0)  # as deafult threshold=0\n",
    "vth.fit(X_train)\n",
    "X_train_vth = X_train.iloc[:, vth.get_support()]\n",
    "\n",
    "pd.DataFrame( {'Feature': X_train.columns,'Variance': vth.variances_,}).sort_values('Variance', ascending=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- None of our features have a zero variance, for that reason we didn't remove any of our features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "<font color=\"lightseagreen\" size=+1><b>Select KBest for Classification Problems</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T15:48:51.270027Z",
     "iopub.status.busy": "2021-09-23T15:48:51.269785Z",
     "iopub.status.idle": "2021-09-23T15:48:52.312364Z",
     "shell.execute_reply": "2021-09-23T15:48:52.311474Z",
     "shell.execute_reply.started": "2021-09-23T15:48:51.269999Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "X= df_heart.drop('DEATH_EVENT', axis=1)\n",
    "y= df_heart['DEATH_EVENT']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.30)\n",
    "\n",
    "Kbest_classif = SelectKBest(score_func=f_classif, k=6)\n",
    "Kbest_classif.fit(X_train, y_train)\n",
    "\n",
    "# what are scores for the features\n",
    "for i in range(len(Kbest_classif.scores_)):\n",
    "    print(f'Feature {i} : {round(Kbest_classif.scores_[i],3)}')\n",
    "\n",
    "print()\n",
    "\n",
    "plt.bar([X_train.columns[i] for i in range(len(Kbest_classif.scores_))], Kbest_classif.scores_)\n",
    "plt.xticks(rotation=90)\n",
    "plt.rcParams[\"figure.figsize\"] = (8,6)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Based on the SelectKBest, ['age','ejection_fraction','serum_creatinine', 'serum_sodium','sex','time'] are selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T15:48:52.31399Z",
     "iopub.status.busy": "2021-09-23T15:48:52.313742Z",
     "iopub.status.idle": "2021-09-23T15:48:52.32789Z",
     "shell.execute_reply": "2021-09-23T15:48:52.326635Z",
     "shell.execute_reply.started": "2021-09-23T15:48:52.313959Z"
    }
   },
   "outputs": [],
   "source": [
    "# transform training set\n",
    "X_train_classif = Kbest_classif.transform(X_train)\n",
    "print(\"X_train.shape: {}\".format(X_train.shape))\n",
    "print()\n",
    "print(\"X_train_selected.shape: {}\".format(X_train_classif.shape))\n",
    "print()\n",
    "# transform test data\n",
    "X_test_classif = Kbest_classif.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's see the differences with the whole features and the selected 6 features by using Logistic Regression as a base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T15:48:52.329834Z",
     "iopub.status.busy": "2021-09-23T15:48:52.329568Z",
     "iopub.status.idle": "2021-09-23T15:48:52.353449Z",
     "shell.execute_reply": "2021-09-23T15:48:52.35254Z",
     "shell.execute_reply.started": "2021-09-23T15:48:52.329805Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "lor = LogisticRegression(solver='liblinear', random_state=0)\n",
    "lor.fit(X_train, y_train)\n",
    "\n",
    "print(f'Score with all features: {round(lor.score(X_test, y_test),4)}')\n",
    "\n",
    "lor.fit(X_train_classif, y_train)\n",
    "\n",
    "print(f'Score with only selected features: {round(lor.score(X_test_classif, y_test),4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this example, by using only 6 features of the dataset, we got better score than by using whole features.\n",
    "- We can suspect that maybe some of the features are uninformative and not providing much about the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "<font color=\"lightseagreen\" size=+1><b>Select KBest for Regression Problems</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T15:48:52.355719Z",
     "iopub.status.busy": "2021-09-23T15:48:52.354625Z",
     "iopub.status.idle": "2021-09-23T15:48:52.609282Z",
     "shell.execute_reply": "2021-09-23T15:48:52.608586Z",
     "shell.execute_reply.started": "2021-09-23T15:48:52.355671Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "X= df_car.drop('price', axis=1)\n",
    "y= df_car['price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.30)\n",
    "\n",
    "Kbest_reg = SelectKBest(score_func=f_regression, k=6)\n",
    "Kbest_reg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# what are scores for the features\n",
    "for i in range(len(Kbest_reg.scores_)):\n",
    "    print(f'Feature {i} : {round(Kbest_reg.scores_[i],3)}')\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "# plot the scores\n",
    "plt.bar([X_train.columns[i] for i in range(len(Kbest_reg.scores_))], Kbest_reg.scores_)\n",
    "plt.xticks(rotation=90)\n",
    "plt.rcParams[\"figure.figsize\"] = (8,6)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T15:48:52.611318Z",
     "iopub.status.busy": "2021-09-23T15:48:52.610354Z",
     "iopub.status.idle": "2021-09-23T15:48:52.621263Z",
     "shell.execute_reply": "2021-09-23T15:48:52.620623Z",
     "shell.execute_reply.started": "2021-09-23T15:48:52.611264Z"
    }
   },
   "outputs": [],
   "source": [
    "# transform training set\n",
    "X_train_reg = Kbest_reg.transform(X_train)\n",
    "print(\"X_train.shape: {}\".format(X_train.shape))\n",
    "print()\n",
    "print(\"X_train_selected.shape: {}\".format(X_train_reg.shape))\n",
    "print()\n",
    "# transform test data\n",
    "X_test_reg = Kbest_reg.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T15:48:52.623146Z",
     "iopub.status.busy": "2021-09-23T15:48:52.622201Z",
     "iopub.status.idle": "2021-09-23T15:48:52.64338Z",
     "shell.execute_reply": "2021-09-23T15:48:52.642368Z",
     "shell.execute_reply.started": "2021-09-23T15:48:52.623109Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "print(f'Score with all features: {round(lr.score(X_test, y_test),4)}')\n",
    "\n",
    "lr.fit(X_train_reg, y_train)\n",
    "\n",
    "print(f'Score with only selected features: {round(lr.score(X_test_reg, y_test),4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bu using only 6 of the features we couldn't get the better score. \n",
    "- Maybe other features on the dataset are informative about the target, we shoudl include them into our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "<font color=\"lightseagreen\" size=+1><b>Information Gain - Classification Problems</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T15:48:52.645876Z",
     "iopub.status.busy": "2021-09-23T15:48:52.645534Z",
     "iopub.status.idle": "2021-09-23T15:48:52.96266Z",
     "shell.execute_reply": "2021-09-23T15:48:52.961584Z",
     "shell.execute_reply.started": "2021-09-23T15:48:52.645833Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "X= df_heart.drop('DEATH_EVENT', axis=1)\n",
    "y= df_heart['DEATH_EVENT']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.30)\n",
    "\n",
    "infogain_classif = SelectKBest(score_func=mutual_info_classif, k=6)\n",
    "\n",
    "infogain_classif.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# what are scores for the features\n",
    "for i in range(len(infogain_classif.scores_)):\n",
    "    print(f'Feature {i} : {round(infogain_classif.scores_[i],3)}')\n",
    "\n",
    "\n",
    "print()\n",
    "\n",
    "# plot the scores\n",
    "plt.bar([X_train.columns[i] for i in range(len(infogain_classif.scores_))], infogain_classif.scores_)\n",
    "plt.xticks(rotation=90)\n",
    "plt.rcParams[\"figure.figsize\"] = (8,6)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T15:48:52.966021Z",
     "iopub.status.busy": "2021-09-23T15:48:52.965743Z",
     "iopub.status.idle": "2021-09-23T15:48:52.976943Z",
     "shell.execute_reply": "2021-09-23T15:48:52.975964Z",
     "shell.execute_reply.started": "2021-09-23T15:48:52.965987Z"
    }
   },
   "outputs": [],
   "source": [
    "# transform training set\n",
    "X_train_info_classif = infogain_classif.transform(X_train)\n",
    "\n",
    "print(\"X_train.shape: {}\".format(X_train.shape))\n",
    "print()\n",
    "\n",
    "print(\"X_train_selected.shape: {}\".format(X_train_info_classif.shape))\n",
    "print()\n",
    "\n",
    "# transform test data\n",
    "X_test_info_classif = infogain_classif.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T15:48:52.978746Z",
     "iopub.status.busy": "2021-09-23T15:48:52.9784Z",
     "iopub.status.idle": "2021-09-23T15:48:52.998792Z",
     "shell.execute_reply": "2021-09-23T15:48:52.997912Z",
     "shell.execute_reply.started": "2021-09-23T15:48:52.978714Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "lor = LogisticRegression(solver='liblinear', random_state=0)\n",
    "lor.fit(X_train, y_train)\n",
    "\n",
    "print(f'Score with all features: {round(lor.score(X_test, y_test),4)}')\n",
    "\n",
    "lor.fit(X_train_info_classif, y_train)\n",
    "\n",
    "print(f'Score with only selected features: {round(lor.score(X_test_info_classif, y_test),4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this example, by using only 6 features of the dataset, we got better score than by using whole features.\n",
    "- We can suspect that maybe some of the features are uninformative and not providing much about the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "<font color=\"lightseagreen\" size=+1><b>Information Gain- Regression Problems</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T15:48:53.000363Z",
     "iopub.status.busy": "2021-09-23T15:48:53.000093Z",
     "iopub.status.idle": "2021-09-23T15:48:53.301454Z",
     "shell.execute_reply": "2021-09-23T15:48:53.300569Z",
     "shell.execute_reply.started": "2021-09-23T15:48:53.00033Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "X= df_car.drop('price', axis=1)\n",
    "y= df_car['price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.30)\n",
    "\n",
    "infogain_reg = SelectKBest(score_func=mutual_info_regression, k=6)\n",
    "\n",
    "infogain_reg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# what are scores for the features\n",
    "for i in range(len(infogain_reg.scores_)):\n",
    "    print('Feature %d: %f' % (i, infogain_reg.scores_[i]))\n",
    "\n",
    "\n",
    "# plot the scores\n",
    "plt.bar([X_train.columns[i] for i in range(len(infogain_reg.scores_))], infogain_reg.scores_)\n",
    "plt.xticks(rotation=90)\n",
    "plt.rcParams[\"figure.figsize\"] = (8,6)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T15:48:53.303695Z",
     "iopub.status.busy": "2021-09-23T15:48:53.302763Z",
     "iopub.status.idle": "2021-09-23T15:48:53.316086Z",
     "shell.execute_reply": "2021-09-23T15:48:53.314956Z",
     "shell.execute_reply.started": "2021-09-23T15:48:53.303645Z"
    }
   },
   "outputs": [],
   "source": [
    "# transform training set\n",
    "X_train_reg = infogain_reg.transform(X_train)\n",
    "print(\"X_train.shape: {}\".format(X_train.shape))\n",
    "print()\n",
    "print(\"X_train_reg.shape: {}\".format(X_train_reg.shape))\n",
    "print()\n",
    "# transform test data\n",
    "X_test_reg = infogain_reg.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T15:48:53.317925Z",
     "iopub.status.busy": "2021-09-23T15:48:53.317682Z",
     "iopub.status.idle": "2021-09-23T15:48:53.338186Z",
     "shell.execute_reply": "2021-09-23T15:48:53.337111Z",
     "shell.execute_reply.started": "2021-09-23T15:48:53.317897Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(f'Score with all features: {round(lr.score(X_test, y_test),4)}')\n",
    "lr.fit(X_train_reg, y_train)\n",
    "print(f'Score with only selected features: {round(lr.score(X_test_reg, y_test),4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a>\n",
    "<font color=\"lightseagreen\" size=+1><b>Select Percentile</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T15:48:53.340109Z",
     "iopub.status.busy": "2021-09-23T15:48:53.33976Z",
     "iopub.status.idle": "2021-09-23T15:48:53.3596Z",
     "shell.execute_reply": "2021-09-23T15:48:53.35831Z",
     "shell.execute_reply.started": "2021-09-23T15:48:53.340066Z"
    }
   },
   "outputs": [],
   "source": [
    "X= df_heart.drop('DEATH_EVENT', axis=1)\n",
    "y= df_heart['DEATH_EVENT']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.30)\n",
    "\n",
    "percentile = SelectPercentile(percentile=50)\n",
    "percentile.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T15:48:53.361466Z",
     "iopub.status.busy": "2021-09-23T15:48:53.360584Z",
     "iopub.status.idle": "2021-09-23T15:48:53.374044Z",
     "shell.execute_reply": "2021-09-23T15:48:53.373144Z",
     "shell.execute_reply.started": "2021-09-23T15:48:53.361423Z"
    }
   },
   "outputs": [],
   "source": [
    "percentile.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T15:48:53.375367Z",
     "iopub.status.busy": "2021-09-23T15:48:53.375139Z",
     "iopub.status.idle": "2021-09-23T15:48:53.390679Z",
     "shell.execute_reply": "2021-09-23T15:48:53.390014Z",
     "shell.execute_reply.started": "2021-09-23T15:48:53.375341Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that **'age','ejection_fraction','serum_creatinine', 'serum_sodium', 'sex', 'time'** are selected by percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T15:48:53.392241Z",
     "iopub.status.busy": "2021-09-23T15:48:53.391997Z",
     "iopub.status.idle": "2021-09-23T15:48:53.412136Z",
     "shell.execute_reply": "2021-09-23T15:48:53.411046Z",
     "shell.execute_reply.started": "2021-09-23T15:48:53.392213Z"
    }
   },
   "outputs": [],
   "source": [
    "# transform training set\n",
    "X_train_percentile = percentile.transform(X_train)\n",
    "print(\"X_train.shape: {}\".format(X_train.shape))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"X_train_selected.shape: {}\".format(X_train_percentile.shape))\n",
    "print()\n",
    "\n",
    "# transform test data\n",
    "X_test_percentile = percentile.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T15:48:53.414918Z",
     "iopub.status.busy": "2021-09-23T15:48:53.414327Z",
     "iopub.status.idle": "2021-09-23T15:48:53.437118Z",
     "shell.execute_reply": "2021-09-23T15:48:53.435928Z",
     "shell.execute_reply.started": "2021-09-23T15:48:53.414861Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "lor = LogisticRegression(solver='liblinear', random_state=0)\n",
    "lor.fit(X_train, y_train)\n",
    "\n",
    "print(f'Score with all features: {round(lor.score(X_test, y_test),4)}')\n",
    "\n",
    "lor.fit(X_train_percentile, y_train)\n",
    "\n",
    "print(f'Score with only selected features: {round(lor.score(X_test_percentile, y_test),4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this example, by using only 6 features of the dataset, we got better score than by using whole features.\n",
    "- We can suspect that maybe some of the features are uninformative and not providing much about the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the filter model the selection of features is done as a preprocessing activity.\n",
    "- We haven't tried to optimize the performance. \n",
    "- By using filter model,we select a subset of features that maximizes  model function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"9\"></a>\n",
    "<font color=\"lightseagreen\" size=+2.5><b>Model-Based Feature Selection - Embedded</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model based feature selection uses ML algortihms to determine the importance of the feature\n",
    "\n",
    "- After then, it keeps the only important ones.\n",
    "- We will use Random Forest Classifier to select the features\n",
    "- We will continue to use Logistic Regression as our base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T15:48:53.438957Z",
     "iopub.status.busy": "2021-09-23T15:48:53.438664Z",
     "iopub.status.idle": "2021-09-23T15:48:53.663157Z",
     "shell.execute_reply": "2021-09-23T15:48:53.662233Z",
     "shell.execute_reply.started": "2021-09-23T15:48:53.438929Z"
    }
   },
   "outputs": [],
   "source": [
    "model_based_feature = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42),threshold=\"median\")\n",
    "\n",
    "X= df_heart.drop('DEATH_EVENT', axis=1)\n",
    "y= df_heart['DEATH_EVENT']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.30)\n",
    "\n",
    "\n",
    "model_based_feature.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T15:48:53.665852Z",
     "iopub.status.busy": "2021-09-23T15:48:53.664649Z",
     "iopub.status.idle": "2021-09-23T15:48:53.683852Z",
     "shell.execute_reply": "2021-09-23T15:48:53.682829Z",
     "shell.execute_reply.started": "2021-09-23T15:48:53.665796Z"
    }
   },
   "outputs": [],
   "source": [
    "model_based_feature.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T15:48:53.686547Z",
     "iopub.status.busy": "2021-09-23T15:48:53.68547Z",
     "iopub.status.idle": "2021-09-23T15:48:53.703287Z",
     "shell.execute_reply": "2021-09-23T15:48:53.702532Z",
     "shell.execute_reply.started": "2021-09-23T15:48:53.686492Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that **'age',''creatinine_phosphokinase',''ejection_fraction',''platelets',''serum_creatinine', 'time'** are selected by model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T15:48:53.704982Z",
     "iopub.status.busy": "2021-09-23T15:48:53.704597Z",
     "iopub.status.idle": "2021-09-23T15:48:53.747387Z",
     "shell.execute_reply": "2021-09-23T15:48:53.745968Z",
     "shell.execute_reply.started": "2021-09-23T15:48:53.704949Z"
    }
   },
   "outputs": [],
   "source": [
    "# transform training set\n",
    "X_train_mbf = model_based_feature.transform(X_train)\n",
    "print(f'X_train.shape: {X_train.shape}')\n",
    "\n",
    "print()\n",
    "\n",
    "print(f'X_train_selected.shape: {X_train_mbf.shape}')\n",
    "print()\n",
    "\n",
    "# transform test data\n",
    "X_test_mbf = model_based_feature.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T15:48:53.749231Z",
     "iopub.status.busy": "2021-09-23T15:48:53.748991Z",
     "iopub.status.idle": "2021-09-23T15:48:53.782444Z",
     "shell.execute_reply": "2021-09-23T15:48:53.781195Z",
     "shell.execute_reply.started": "2021-09-23T15:48:53.749205Z"
    }
   },
   "outputs": [],
   "source": [
    "lor = LogisticRegression()\n",
    "lor.fit(X_train_mbf, y_train)\n",
    "\n",
    "print(f'Score with only selected features: {round(lor.score(X_test_mbf, y_test),4)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With the better feature selection, we also gained some improvements here.\n",
    "- By using model based feature, even with base model Logistic Regression we get better result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"10\"></a>\n",
    "<font color=\"lightseagreen\" size=+2.5><b>Iterative Feature Selection- Wrapper</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Iterative feature selection uses different models which have different subsets of future variables.\n",
    "- One method, start wiith no features and add features one by one until the criteria defined by the user.\n",
    "- Other method, which we use in our example, RFE (recursive feature elimination),\n",
    "- RFE starts with the all the features and builds the model and eliminates the least important feature from the model.\n",
    "- Just to remember that, since iterative-wrapper method builts several models, it is computationaly expensive to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T15:48:53.784513Z",
     "iopub.status.busy": "2021-09-23T15:48:53.784031Z",
     "iopub.status.idle": "2021-09-23T15:48:55.25801Z",
     "shell.execute_reply": "2021-09-23T15:48:55.256672Z",
     "shell.execute_reply.started": "2021-09-23T15:48:53.784465Z"
    }
   },
   "outputs": [],
   "source": [
    "rfe_features = RFE(RandomForestClassifier(n_estimators=100, random_state=42),n_features_to_select=6)\n",
    "\n",
    "X= df_heart.drop('DEATH_EVENT', axis=1)\n",
    "y= df_heart['DEATH_EVENT']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.30)\n",
    "\n",
    "\n",
    "rfe_features.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T15:48:55.260015Z",
     "iopub.status.busy": "2021-09-23T15:48:55.259711Z",
     "iopub.status.idle": "2021-09-23T15:48:55.268502Z",
     "shell.execute_reply": "2021-09-23T15:48:55.267355Z",
     "shell.execute_reply.started": "2021-09-23T15:48:55.259981Z"
    }
   },
   "outputs": [],
   "source": [
    "rfe_features.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T15:48:55.270762Z",
     "iopub.status.busy": "2021-09-23T15:48:55.270363Z",
     "iopub.status.idle": "2021-09-23T15:48:55.287955Z",
     "shell.execute_reply": "2021-09-23T15:48:55.286789Z",
     "shell.execute_reply.started": "2021-09-23T15:48:55.270712Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that **'age',''creatinine_phosphokinase',''ejection_fraction',''platelets',''serum_creatinine', 'time'** are selected by iterative future selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T15:48:55.290287Z",
     "iopub.status.busy": "2021-09-23T15:48:55.289243Z",
     "iopub.status.idle": "2021-09-23T15:48:55.312006Z",
     "shell.execute_reply": "2021-09-23T15:48:55.309565Z",
     "shell.execute_reply.started": "2021-09-23T15:48:55.290249Z"
    }
   },
   "outputs": [],
   "source": [
    "# transform training set\n",
    "\n",
    "X_train_rfe= rfe_features.transform(X_train)\n",
    "\n",
    "print(f'X_train.shape: {X_train.shape}')\n",
    "\n",
    "print()\n",
    "\n",
    "print(f'X_train_selected.shape: {X_train_rfe.shape}')\n",
    "print()\n",
    "\n",
    "# transform test data\n",
    "X_test_rfe= rfe_features.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-23T15:48:55.314749Z",
     "iopub.status.busy": "2021-09-23T15:48:55.314389Z",
     "iopub.status.idle": "2021-09-23T15:48:55.366048Z",
     "shell.execute_reply": "2021-09-23T15:48:55.364515Z",
     "shell.execute_reply.started": "2021-09-23T15:48:55.314706Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "lor = LogisticRegression()\n",
    "lor.fit(X_train_mbf, y_train)\n",
    "\n",
    "print(f'Score with only selected features: {round(lor.score(X_test_rfe, y_test),4)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With the better feature selection, we also gained some improvements here.\n",
    "- By using model based feature, even with base model Logistic Regression we get better result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"11\"></a>\n",
    "<font color=\"darkblue\" size=+1.5><b>Conclusion & Which Method is the Best?</b></font>\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Traditionaly, feature selection aims to improve accuracy and interpretability  \n",
    "- We can add these aims also algorithmic training time and reducing memory footprint.\n",
    "- Since we are dealing a large samples and thousands and thousand features, feature selection is one of the choice in our hands.\n",
    "- No single data-reduction method can be best suited for all applications. \n",
    "- Available knowledge on the application is the one of the most important things which feature selection is based on.\n",
    "- At the end there is no one best method to offer.\n",
    "- We need to find out best features for our problem by using different methods and algorithms with the systematic experimentation.\n",
    "- Even though in a real life cases, mostly we can not get large gains by applying feature selection, I can assure that it is valuable tool to have in your toolbox.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **By the way, when you like the topic, you can show it by supporting** üëç\n",
    "\n",
    "####  **Feel free to leave a comment in the notebook**. \n",
    "\n",
    "#### All the best ü§ò"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Enjoy** ü§ò"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://media.giphy.com/media/u1PvrqJWlY4OPSxDJ5/giphy-downsized-large.gif?cid=ecf05e47oo0ggho7d7xjvqnbs372rv1apu2eubitqk0ctt9d&rid=giphy-downsized-large.gif&ct=g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"12\"></a>\n",
    "<font color=\"darkblue\" size=+1.5><b>References & Further Reading</b></font>\n",
    "\n",
    "\n",
    "<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>\n",
    "\n",
    "\n",
    "[Machine Learning - Beginner &Intermediate-Friendly BOOKS](https://www.kaggle.com/general/255972)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
